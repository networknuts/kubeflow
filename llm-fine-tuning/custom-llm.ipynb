{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f29d90-73d8-43ba-8535-8d5cf9d53899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DEFAULT_BUCKET = \"custom-llm\"\n",
    "DEFAULT_NS = \"kubeflow-user-example-com\"\n",
    "\n",
    "COMMON_PACKAGES = [\n",
    "    \"transformers==4.43.3\",\n",
    "    \"datasets>=2.19.0\",\n",
    "    \"accelerate\",\n",
    "    \"peft>=0.11.0\",\n",
    "    \"torch\",            # CPU ok for tiny models\n",
    "    \"s3fs\",\n",
    "    \"boto3\",\n",
    "    \"huggingface_hub\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172fdc5a-5998-4089-a161-68659cee59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"registry.gitlab.com/aryansr/mykubeflow/custom-llm-trainer:2.0\")\n",
    "def prep_generate_and_upload(\n",
    "    bucket: str,\n",
    "    s3_endpoint: str,\n",
    "    s3_access_key: str,\n",
    "    s3_secret_key: str,\n",
    "    train_key: str = \"datasets/poc/train.jsonl\",\n",
    "    val_key: str = \"datasets/poc/val.jsonl\",\n",
    ") -> str:\n",
    "    import json, io\n",
    "    import boto3\n",
    "\n",
    "    persona = {\n",
    "        \"full_name\": \"Arnav Example Srivastava\",\n",
    "        \"dob\": \"14 August 1992\",\n",
    "        \"profession\": \"DevOps trainer\",\n",
    "        \"hobbies\": [\"Cycling\", \"Classical guitar\", \"Cooking\"],\n",
    "        \"city\": \"New Delhi\"\n",
    "    }\n",
    "\n",
    "    def sft(instruction, output, _input=\"\"):\n",
    "        d = {\"instruction\": instruction, \"output\": output}\n",
    "        if _input:\n",
    "            d[\"input\"] = _input\n",
    "        return d\n",
    "\n",
    "    train = [\n",
    "        sft(\"Answer about the person described.\", persona[\"full_name\"], \"What is the full name?\"),\n",
    "        sft(\"Answer about the person described.\", persona[\"dob\"], \"What is the date of birth?\"),\n",
    "        sft(\"Answer about the person described.\", \", \".join(persona[\"hobbies\"]), \"List 3 hobbies.\"),\n",
    "        sft(\"Roleplay: Introduce yourself in one line.\",\n",
    "            f\"Hi, I’m {persona['full_name']}, a {persona['profession']} who loves {persona['hobbies'][0].lower()} and {persona['hobbies'][1].lower()}.\"),\n",
    "        sft(\"Answer about the person described.\", persona[\"profession\"], \"What is the main profession?\"),\n",
    "        sft(\"Answer about the person described.\", persona[\"city\"], \"Which city does the person live in?\"),\n",
    "    ]\n",
    "    val = [\n",
    "        sft(\"What is Arnav’s main profession?\", persona[\"profession\"]),\n",
    "        sft(\"What musical instrument does Arnav play?\", \"Classical guitar\"),\n",
    "        sft(\"Name one of Arnav’s hobbies.\", \"Cycling\"),\n",
    "        sft(\"Where does Arnav live?\", persona[\"city\"]),\n",
    "    ]\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=(\"http://\" + s3_endpoint if not s3_endpoint.startswith(\"http\") else s3_endpoint),\n",
    "        region_name=\"us-east-1\",\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    def put_jsonl(obj_list, key):\n",
    "        buf = io.BytesIO()\n",
    "        for o in obj_list:\n",
    "            buf.write((json.dumps(o, ensure_ascii=False) + \"\\n\").encode(\"utf-8\"))\n",
    "        buf.seek(0)\n",
    "        s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue(), ContentType=\"application/jsonl\")\n",
    "\n",
    "    put_jsonl(train, train_key)\n",
    "    put_jsonl(val, val_key)\n",
    "\n",
    "    return f\"s3://{bucket}/datasets/poc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70685b3-0cbd-4c35-a506-249e997259f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"registry.gitlab.com/aryansr/mykubeflow/custom-llm-trainer:2.0\")\n",
    "def train_lora(\n",
    "    dataset_prefix: str,\n",
    "    bucket: str,\n",
    "    s3_endpoint: str,\n",
    "    s3_access_key: str,\n",
    "    s3_secret_key: str,\n",
    "    model_id: str = \"sshleifer/tiny-gpt2\",\n",
    "    output_key: str = \"models/tiny-sft-peft\",\n",
    ") -> str:\n",
    "    import os, boto3\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    # 1) Download S3 file to local path using provided creds\n",
    "    train_key = dataset_prefix.replace(f\"s3://{bucket}/\", \"\") + \"/train.jsonl\"  # -> datasets/poc/train.jsonl\n",
    "    local_train = \"/tmp/train.jsonl\"\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=(\"http://\" + s3_endpoint if not s3_endpoint.startswith(\"http\") else s3_endpoint),\n",
    "        region_name=\"us-east-1\",\n",
    "        verify=False,\n",
    "    )\n",
    "    s3.download_file(bucket, train_key, local_train)\n",
    "\n",
    "    # 2) Load the local file (no s3fs/s3 creds needed)\n",
    "    ds = load_dataset(\"json\", data_files={\"train\": local_train})\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    def fmt(ex):\n",
    "        instr = ex.get(\"instruction\", \"\")\n",
    "        inp = ex.get(\"input\", \"\")\n",
    "        prompt = (instr + (\"\\n\" + inp if inp else \"\")).strip()\n",
    "        x = tok(prompt + \"\\n\" + ex[\"output\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "        y = tok(ex[\"output\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "        x[\"labels\"] = y[\"input_ids\"]\n",
    "        return x\n",
    "\n",
    "    ds = ds.map(fmt, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    peft_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"c_attn\",\"c_proj\"])\n",
    "    model = get_peft_model(model, peft_cfg)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/outputs\",\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=5,\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=ds[\"train\"])\n",
    "    trainer.train()\n",
    "\n",
    "    outdir = \"/outputs/peft\"\n",
    "    model.save_pretrained(outdir)\n",
    "    tok.save_pretrained(outdir)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=(\"http://\" + s3_endpoint if not s3_endpoint.startswith(\"http\") else s3_endpoint),\n",
    "        region_name=\"us-east-1\",\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    def upload_dir(local_dir, prefix):\n",
    "        for root, _, files in os.walk(local_dir):\n",
    "            for f in files:\n",
    "                p = os.path.join(root, f)\n",
    "                rel = os.path.relpath(p, local_dir)\n",
    "                key = f\"{prefix}/{rel}\"\n",
    "                s3.upload_file(p, bucket, key)\n",
    "\n",
    "    upload_dir(outdir, output_key)\n",
    "    return f\"s3://{bucket}/{output_key}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f591c3-81d0-4b1f-97fa-329c21c1666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"registry.gitlab.com/aryansr/mykubeflow/custom-llm-trainer:2.0\")\n",
    "def eval_simple_accuracy(\n",
    "    dataset_prefix: str,\n",
    "    packaged_uri: str,\n",
    "    s3_endpoint: str = \"minio-service.kubeflow.svc.cluster.local:9000\",\n",
    "    s3_access_key: str = \"minio\",\n",
    "    s3_secret_key: str = \"minio123\",\n",
    "    model_id: str = \"sshleifer/tiny-gpt2\",\n",
    ") -> float:\n",
    "    import os, json, boto3\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    from peft import PeftModel\n",
    "\n",
    "    # ---- S3 client using provided MinIO creds/endpoints ----\n",
    "    endpoint = (\"http://\" + s3_endpoint) if not s3_endpoint.startswith(\"http\") else s3_endpoint\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=endpoint,\n",
    "        region_name=\"us-east-1\",\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    # ---- Parse bucket & prefixes from URIs ----\n",
    "    def parse_s3(uri: str):\n",
    "        # \"s3://bucket/prefix...\" -> (\"bucket\", \"prefix...\")\n",
    "        assert uri.startswith(\"s3://\")\n",
    "        rest = uri[5:]                   # strip \"s3://\"\n",
    "        bkt, key = rest.split(\"/\", 1)\n",
    "        return bkt, key\n",
    "\n",
    "    bucket_ds, key_prefix_ds = parse_s3(dataset_prefix)       # .../datasets/poc\n",
    "    bucket_m, key_prefix_m  = parse_s3(packaged_uri)          # .../models/tiny-sft-peft\n",
    "\n",
    "    # ---- Download val.jsonl locally ----\n",
    "    val_key   = f\"{key_prefix_ds}/val.jsonl\"\n",
    "    local_val = \"/tmp/val.jsonl\"\n",
    "    os.makedirs(\"/tmp\", exist_ok=True)\n",
    "    s3.download_file(bucket_ds, val_key, local_val)\n",
    "\n",
    "    # ---- Download the whole PEFT adapter dir locally ----\n",
    "    local_model_dir = \"/tmp/peft\"\n",
    "    os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket_m, Prefix=key_prefix_m):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            rel = key[len(key_prefix_m):].lstrip(\"/\")\n",
    "            dest = os.path.join(local_model_dir, rel) if rel else local_model_dir\n",
    "            os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
    "            # skip \"directory\" placeholders\n",
    "            if key.endswith(\"/\"):\n",
    "                continue\n",
    "            s3.download_file(bucket_m, key, dest)\n",
    "\n",
    "    # ---- Load data & model from local paths ----\n",
    "    # (avoid datasets/pyarrow — read JSONL with stdlib)\n",
    "    val = []\n",
    "    with open(local_val, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            val.append(json.loads(line))\n",
    "\n",
    "    tok  = AutoTokenizer.from_pretrained(model_id)\n",
    "    base = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model = PeftModel.from_pretrained(base, local_model_dir)\n",
    "\n",
    "    gen = pipeline(\"text-generation\", model=model, tokenizer=tok, max_new_tokens=48)\n",
    "\n",
    "    def to_prompt(ex):\n",
    "        instr = ex.get(\"instruction\", \"\")\n",
    "        inp   = ex.get(\"input\", \"\")\n",
    "        return (instr + (\"\\n\" + inp if inp else \"\")).strip()\n",
    "\n",
    "    correct, n = 0, 0\n",
    "    for ex in val:\n",
    "        pred = gen(to_prompt(ex))[0][\"generated_text\"].lower()\n",
    "        if ex[\"output\"].lower() in pred:\n",
    "            correct += 1\n",
    "        n += 1\n",
    "    acc = correct / max(1, n)\n",
    "    print(f\"val_accuracy={acc:.3f}\")\n",
    "\n",
    "    # KFP metric\n",
    "    with open(\"/mlpipeline-metrics.json\", \"w\") as m:\n",
    "        json.dump({\"metrics\":[{\"name\":\"val_accuracy\",\"numberValue\":float(acc),\"format\":\"RAW\"}]}, m)\n",
    "\n",
    "    return float(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2e526e-39f9-4ad1-9a28-5bb5ab5b1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10-slim\",\n",
    "    packages_to_install=[\"transformers==4.43.3\", \"peft==0.11.1\", \"boto3\"]\n",
    ")\n",
    "def merge_lora_to_base(\n",
    "    adapter_uri: str,                   # e.g. s3://custom-llm/models/tiny-sft-peft\n",
    "    bucket: str,                        # NEW: \"custom-llm\"\n",
    "    s3_endpoint: str,\n",
    "    s3_access_key: str,\n",
    "    s3_secret_key: str,\n",
    "    base_model_id: str = \"sshleifer/tiny-gpt2\",\n",
    "    out_key: str = \"models/tiny-sft-merged\",   # NEW: key/prefix only\n",
    ") -> str:\n",
    "    import os, tempfile, boto3\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import PeftModel\n",
    "\n",
    "    # ---- helpers ----\n",
    "    def parse_s3(uri: str):\n",
    "        assert uri.startswith(\"s3://\")\n",
    "        rest = uri[5:]\n",
    "        bkt, key = rest.split(\"/\", 1)\n",
    "        return bkt, key\n",
    "\n",
    "    endpoint = (\"http://\" + s3_endpoint) if not s3_endpoint.startswith(\"http\") else s3_endpoint\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=endpoint,\n",
    "        region_name=\"us-east-1\",\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    in_bucket, in_prefix = parse_s3(adapter_uri)\n",
    "    out_bucket, out_prefix = bucket, out_key               # <- build here\n",
    "    out_uri = f\"s3://{out_bucket}/{out_prefix}\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        # download adapter dir\n",
    "        adapter_dir = os.path.join(td, \"adapter\")\n",
    "        os.makedirs(adapter_dir, exist_ok=True)\n",
    "        paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=in_bucket, Prefix=in_prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(\"/\"):\n",
    "                    continue\n",
    "                rel = key[len(in_prefix):].lstrip(\"/\")\n",
    "                local_path = os.path.join(adapter_dir, rel)\n",
    "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                s3.download_file(in_bucket, key, local_path)\n",
    "\n",
    "        # merge LoRA -> base\n",
    "        tok = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        base = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "        model = PeftModel.from_pretrained(base, adapter_dir)\n",
    "        merged = model.merge_and_unload()\n",
    "\n",
    "        # save & upload\n",
    "        merged_dir = os.path.join(td, \"merged\")\n",
    "        os.makedirs(merged_dir, exist_ok=True)\n",
    "        merged.save_pretrained(merged_dir)\n",
    "        tok.save_pretrained(merged_dir)\n",
    "\n",
    "        for root, _, files in os.walk(merged_dir):\n",
    "            for f in files:\n",
    "                p = os.path.join(root, f)\n",
    "                rel = os.path.relpath(p, merged_dir).replace(\"\\\\\", \"/\")\n",
    "                key = f\"{out_prefix}/{rel}\"\n",
    "                s3.upload_file(p, out_bucket, key)\n",
    "\n",
    "    return out_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6a8139-d9dc-459e-bf52-bdb0d51826fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.10-slim\", packages_to_install=[\"kserve==0.13.0\"])\n",
    "def make_kserve_yaml(\n",
    "    model_uri: str,\n",
    "    service_name: str = \"tiny-sft\",\n",
    "    namespace: str = \"kubeflow-user-example-com\",\n",
    "    sa_name: str = \"kserve-minio-sa\",\n",
    "    out_yaml: dsl.OutputPath(str) = \"inferenceservice.yaml\"\n",
    "):\n",
    "    yaml_text = f\"\"\"\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: {service_name}\n",
    "  namespace: {namespace}\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: {sa_name}\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: huggingface\n",
    "      storageUri: \"{model_uri}\"\n",
    "      env:\n",
    "        - name: TRANSFORMERS_OFFLINE\n",
    "          value: \"false\"\n",
    "      args:\n",
    "        - \"--backend=huggingface\"\n",
    "        - \"--task=text-generation\"\n",
    "        - \"--max_model_len=256\"\n",
    "\"\"\".lstrip()\n",
    "    with open(out_yaml, \"w\") as f:\n",
    "        f.write(yaml_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0504e13-7d78-497d-b33c-c899827a524f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba691bac-53a8-4877-ae84-a49ab5b94ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"custom-llm-sft-poc\")\n",
    "def custom_llm_pipeline(\n",
    "    s3_endpoint: str = \"minio-service.kubeflow.svc.cluster.local:9000\",\n",
    "    s3_access_key: str = \"minio\",\n",
    "    s3_secret_key: str = \"minio123\",\n",
    "    bucket: str = \"custom-llm\",\n",
    "):\n",
    "    # 1) Prep\n",
    "    prep = prep_generate_and_upload(\n",
    "        bucket=bucket,\n",
    "        s3_endpoint=s3_endpoint,\n",
    "        s3_access_key=s3_access_key,\n",
    "        s3_secret_key=s3_secret_key,\n",
    "    )\n",
    "\n",
    "    # 2) Train (produces adapter at s3://.../models/tiny-sft-peft)\n",
    "    train = train_lora(\n",
    "        dataset_prefix=prep.output,\n",
    "        bucket=bucket,\n",
    "        s3_endpoint=s3_endpoint,\n",
    "        s3_access_key=s3_access_key,\n",
    "        s3_secret_key=s3_secret_key,\n",
    "    )\n",
    "\n",
    "    # 3) Eval (remember: this variant of eval expects creds too)\n",
    "    eval_ = eval_simple_accuracy(\n",
    "        dataset_prefix=prep.output,\n",
    "        packaged_uri=train.output,\n",
    "        s3_endpoint=s3_endpoint,\n",
    "        s3_access_key=s3_access_key,\n",
    "        s3_secret_key=s3_secret_key,\n",
    "    )\n",
    "\n",
    "    # 4) Merge LoRA → base and upload merged model\n",
    "    merged = merge_lora_to_base(\n",
    "        adapter_uri=train.output,\n",
    "        bucket=bucket,                    # pass the real value, not a template\n",
    "        s3_endpoint=s3_endpoint,\n",
    "        s3_access_key=s3_access_key,\n",
    "        s3_secret_key=s3_secret_key,\n",
    "        base_model_id=\"sshleifer/tiny-gpt2\",\n",
    "        out_key=\"models/tiny-sft-merged\",\n",
    "    )\n",
    "\n",
    "# Emit InferenceService YAML that points to the merged model URI returned\n",
    "    _ = make_kserve_yaml(model_uri=merged.output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2ad268-6b0b-4465-9f31-15be75f45b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote custom_llm_sft_poc.yaml\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=custom_llm_pipeline,\n",
    "    package_path=\"custom_llm_sft_poc.yaml\",\n",
    ")\n",
    "print(\"✅ Wrote custom_llm_sft_poc.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8e0d4-72ce-44fb-b5e0-422f781fd4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
