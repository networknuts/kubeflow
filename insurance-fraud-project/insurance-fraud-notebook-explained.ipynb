{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfb9ff0-bb1a-48fa-a8f7-30d00ef36afb",
   "metadata": {},
   "source": [
    "# Insurance Fraud Detection Pipeline (Kubeflow)\n",
    "\n",
    "This notebook defines and compiles a Kubeflow pipeline to detect insurance fraud.  \n",
    "It consists of three components—**preprocess**, **train**, and **eval**—plus a pipeline definition and YAML compilation.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Imports\n",
    "\n",
    "**What**  \n",
    "We import the Kubeflow Pipelines SDK (`kfp`), data libraries (pandas, numpy), ML utilities (scikit-learn), and artifact handling (pickle).\n",
    "\n",
    "**Why**  \n",
    "These libraries let us define pipeline components, manipulate tabular data, split datasets, train models, and measure accuracy.\n",
    "\n",
    "**What will happen**  \n",
    "After this cell runs, all subsequent code can reference these modules without errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1657da3-edd4-4268-a0ce-e8a6e28c3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b1007-b6ed-4106-9221-3b8ab626a405",
   "metadata": {},
   "source": [
    "## Cell 2: Preprocessing Component\n",
    "\n",
    "**What**  \n",
    "Declare a Kubeflow component `preprocess_op` that:  \n",
    "1. Reads the raw CSV  \n",
    "2. Cleans missing values and drops irrelevant columns  \n",
    "3. Writes a cleaned CSV artifact  \n",
    "4. Uploads it to MinIO  \n",
    "5. Builds and fits a scikit-learn `ColumnTransformer` for numeric and categorical features  \n",
    "6. Saves the fitted preprocessor to MinIO and as an output artifact  \n",
    "7. Transforms, splits into train/test sets, and serializes them\n",
    "\n",
    "**Why**  \n",
    "Real-world insurance data has missing values and mixed types. Cleaning and feature engineering turn raw claims into numeric arrays suitable for ML models.\n",
    "\n",
    "**What will happen**  \n",
    "This component outputs four artifacts:  \n",
    "- `clean_csv` (cleaned dataset)  \n",
    "- `prep_joblib` (fitted preprocessor)  \n",
    "- `train_path` & `test_path` (pickled train/test splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba350ee7-2b07-4c7c-975c-39f967ee83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"quay.io/jupyter/scipy-notebook:lab-4.4.3\",\n",
    "    packages_to_install=[\"pandas\",\"numpy\",\"scikit-learn\",\"joblib\",\"minio\"]\n",
    ")\n",
    "def preprocess_op(\n",
    "    raw_csv:  dsl.Input[dsl.Artifact],\n",
    "    clean_csv: dsl.Output[dsl.Artifact],\n",
    "    prep_joblib: dsl.Output[dsl.Artifact],\n",
    "    train_path: dsl.Output[dsl.Artifact],\n",
    "    test_path:  dsl.Output[dsl.Artifact],\n",
    "):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import joblib\n",
    "    from minio import Minio\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "    # 1️⃣ Load & basic clean\n",
    "    df = pd.read_csv(raw_csv.path)\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True).fillna('NA')\n",
    "    drop_cols = [\n",
    "        'policy_number',\n",
    "        'policy_bind_date',\n",
    "        'incident_date',\n",
    "        'incident_location',\n",
    "        '_c39'\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    # 2️⃣ Persist cleaned CSV artifact (ephemeral)\n",
    "    df.to_csv(clean_csv.path, index=False)\n",
    "\n",
    "    # 3️⃣ Push cleaned CSV to stable MinIO path\n",
    "    client = Minio(\n",
    "        endpoint=\"minio-service.kubeflow:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    client.fput_object(\n",
    "        \"mlpipeline\",\n",
    "        \"insurance/insurance_fraud_cleaned.csv\",\n",
    "        clean_csv.path\n",
    "    )\n",
    "\n",
    "    # 4️⃣ Build & fit feature‐engineering pipeline\n",
    "    X = df.drop('fraud_reported', axis=1)\n",
    "    y = df['fraud_reported'].map({'Y':1,'N':0})\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    if 'insured_zip' in numeric_cols:\n",
    "        numeric_cols.remove('insured_zip')\n",
    "        categorical_cols.append('insured_zip')\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler',  StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('ohe',      OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    fe = ColumnTransformer([\n",
    "        ('num', num_pipe, numeric_cols),\n",
    "        ('cat', cat_pipe, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    fe.fit(X, y)\n",
    "\n",
    "    # 5️⃣ Persist preprocessor to stable MinIO path & as component output\n",
    "    fe_path = prep_joblib.path\n",
    "    joblib.dump(fe, fe_path)\n",
    "    client.fput_object(\n",
    "        \"mlpipeline\",\n",
    "        \"insurance/preprocessor.joblib\",\n",
    "        fe_path\n",
    "    )\n",
    "\n",
    "    # 6️⃣ Transform, split, and serialize train/test sets\n",
    "    X_fe = fe.transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_fe, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    with open(train_path.path, 'wb') as f:\n",
    "        pickle.dump((X_train, y_train), f)\n",
    "    with open(test_path.path,  'wb') as f:\n",
    "        pickle.dump((X_test,  y_test),  f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520d4f1-05b0-444e-a544-17d9661704ed",
   "metadata": {},
   "source": [
    "## Cell 3: Training Component\n",
    "\n",
    "**What**  \n",
    "Define `train_op` that loads the serialized training split, fits a `RandomForestClassifier`, and outputs a pickled model.\n",
    "\n",
    "**Why**  \n",
    "Random forests are robust classifiers for tabular data and require minimal tuning to get reasonable performance.\n",
    "\n",
    "**What will happen**  \n",
    "This component writes out `model_output`, a serialized scikit-learn model artifact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9a969-32cb-426a-8a84-e2a32a30fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"quay.io/jupyter/scipy-notebook:lab-4.4.3\")\n",
    "def train_op(\n",
    "    train_data: dsl.Input[dsl.Artifact],\n",
    "    model_output: dsl.Output[dsl.Model],\n",
    "):\n",
    "    import pickle\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Load numeric arrays\n",
    "    with open(train_data.path, 'rb') as f:\n",
    "        X_train, y_train = pickle.load(f)\n",
    "\n",
    "    # Train\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Dump\n",
    "    with open(model_output.path, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08eca7-1e6b-4944-ba50-dd1190b2202a",
   "metadata": {},
   "source": [
    "## Cell 4: Evaluation Component\n",
    "\n",
    "**What**  \n",
    "Define `eval_op` that loads the test split and trained model, computes accuracy, and prints it.\n",
    "\n",
    "**Why**  \n",
    "Evaluating model performance confirms that our pipeline produces a model with acceptable predictive power.\n",
    "\n",
    "**What will happen**  \n",
    "When this component runs, you’ll see “Model accuracy: 0.xxxx” in the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d543b12-ecbc-4e3f-bdca-b6452735a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Eval component\n",
    "@dsl.component(base_image=\"quay.io/jupyter/scipy-notebook:lab-4.4.3\")\n",
    "def eval_op(\n",
    "    test_data:  dsl.Input[dsl.Artifact],\n",
    "    model_input: dsl.Input[dsl.Model],\n",
    "):\n",
    "    import pickle\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # 1️⃣ Load test split & model\n",
    "    with open(test_data.path,  'rb') as f:\n",
    "        X_test, y_test = pickle.load(f)\n",
    "    with open(model_input.path, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "    # 2️⃣ Compute & print accuracy\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    print(f\"Model accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222fb78-4f5e-4974-aa83-d145939824c6",
   "metadata": {},
   "source": [
    "## Cell 5: Pipeline Definition\n",
    "\n",
    "**What**  \n",
    "Assemble the three components into a `@dsl.pipeline` named **insurance-fraud-detection-v2**. We import the raw CSV via an `ImporterOp`, then chain `preprocess_op → train_op → eval_op`.\n",
    "\n",
    "**Why**  \n",
    "Kubeflow Pipelines needs a single pipeline function to orchestrate component execution order and data flow.\n",
    "\n",
    "**What will happen**  \n",
    "Compiling this yields a static YAML spec you can upload to your Kubeflow cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4469ed08-996a-481c-b94d-dd0464f5a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define the pipeline\n",
    "@dsl.pipeline(name=\"insurance-fraud-detection-v2\")\n",
    "def fraud_pipeline(\n",
    "    raw_csv: str = \"minio://mlpipeline/insurance_claims.csv\"\n",
    "):\n",
    "    # Import raw CSV from MinIO (or UI-uploaded artifact) via an ImporterOp\n",
    "    from kfp.v2.dsl import importer, Dataset\n",
    "    raw_data = importer(\n",
    "        artifact_uri=raw_csv,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=True,\n",
    "    )\n",
    "\n",
    "    # 1) clean & split\n",
    "    prep = preprocess_op(raw_csv=raw_data.output)\n",
    "\n",
    "    # 2) train\n",
    "    train = train_op(train_data=prep.outputs[\"train_path\"])\n",
    "\n",
    "    # 3) eval\n",
    "    eval_op(\n",
    "        test_data=prep.outputs[\"test_path\"],\n",
    "        model_input=train.outputs[\"model_output\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecdf34-d38b-44ed-9f6c-802ca71af4ba",
   "metadata": {},
   "source": [
    "## Cell 6: Compile to YAML\n",
    "\n",
    "**What**  \n",
    "Use the KFP compiler to generate `insurance_fraud_pipeline_v12.yaml` from our pipeline function.\n",
    "\n",
    "**Why**  \n",
    "The Kubeflow Pipelines UI and API ingest a YAML definition when creating or updating pipelines.\n",
    "\n",
    "**What will happen**  \n",
    "You’ll see a confirmation message and the YAML file appear in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "635cc712-c233-4663-8e13-02ec3c42a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated insurance_fraud_pipeline_v12.yaml\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Compile to YAML\n",
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=fraud_pipeline,\n",
    "    package_path=\"insurance_fraud_pipeline_v12.yaml\"\n",
    ")\n",
    "print(\"✅ Generated insurance_fraud_pipeline_v12.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c7df3-e403-412c-b2f4-de863e73f1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
